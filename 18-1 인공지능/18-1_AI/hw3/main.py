# -*- coding: utf-8 -*-
"""AI_HW3_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16QisGYqaAm_TUf5AHFOKUmwV0LuyBFM-
"""

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)

import tensorflow as tf

# Parameters
learning_rate = 0.01
training_epochs = 20
batch_size = 100
display_step = 1

X = tf.placeholder("float", [None, 784])

Y = tf.placeholder("float", [None, 10])

weights = {
    'h_conv1': tf.Variable(tf.random_normal([5, 5, 1, 4],mean=0, stddev=0.1)),
    'h_conv2': tf.Variable(tf.random_normal([5, 5, 4, 12], mean=0, stddev=0.1)),
    'h_fc': tf.Variable(tf.random_normal([4*4*12, 10]))
}

biases = {
    'b_conv1': tf.Variable(tf.random_normal([4], mean =0, stddev=0.1)),
    'b_conv2': tf.Variable(tf.random_normal([12], mean=0, stddev=0.1)),
    'b_fc': tf.Variable(tf.random_normal([10]))
}

def lenet_mnist(x):
    x_image = tf.reshape(x, [-1, 28, 28, 1])
    conv1 = tf.nn.relu(tf.nn.conv2d(x_image, weights['h_conv1'], strides =[1,1,1,1], padding="VALID")
                       + biases['b_conv1'])
    pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
    
    conv2 = tf.nn.relu(tf.nn.conv2d(pool1, weights['h_conv2'], strides=[1,1,1,1], padding="VALID")
                       +biases['b_conv2'])
    pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
    pool2_flat = tf.reshape(pool2, [-1,4*4*12])
    
    fc = tf.matmul(pool2_flat, weights['h_fc']) + biases['b_fc']
    return fc

logits = lenet_mnist(X)

loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)
init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(mnist.train.num_examples/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})
            avg_cost += c / total_batch
        if epoch % display_step == 0:
            print("Train Set: Epoch", '%d' % (epoch+1), "Average Loss: {:.6f}".format(avg_cost), 
                  "lr:{:.2e}".format(learning_rate))
    
    pred = tf.nn.softmax(logits)
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    ac, averageLoss = sess.run([accuracy, loss_op], feed_dict={X: mnist.test.images, Y:mnist.test.labels})
    print("Test Set: Average Loss : {:.6f}".format(averageLoss), "Accuracy:", "%d/10000"%(ac*10000), 
          "({:.2f}%)".format(ac*100))